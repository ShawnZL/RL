我觉得这部分最重要的是理解策略和价值函数，已经推理过程

同时，这个env.P 这个参数从第一次计算后就得到就一直不会变化，相当于我们已知了整体环境，其中在变化的就是v，这个是就是V方程

V是状态价值函数，Q是动作价值函数，但是策略是在状态S下选择动作的概率，所以在我眼中，状态价值函数也是跟动作有关系，只是将这个概念概括为全部动作。

在强化学习中，**价值函数（Value Function）** 用于评估智能体在某个状态（或状态-动作对）下未来能获得的期望回报，是策略评估和优化的核心工具。主要分为两类：

---

### **1. 状态价值函数（State-Value Function, \( V^\pi(s) \)）**
- **定义**：在策略 \( \pi \) 下，从状态 \( s \) 开始，智能体未来获得的**期望回报（Expected Return）**。  
  $$
  V^\pi(s) = \mathbb{E}_\pi \left[ G_t \mid S_t = s \right]
  $$
  其中 \( G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots \) 是折扣回报，\( \gamma \) 是折扣因子。

- **特点**：  
  - 只依赖状态 \( s \) 和策略 \( \pi \)，与动作无关。  
  - 用于评估策略 \( \pi \) 的优劣（例如：比较不同策略在相同状态下的长期收益）。  

- **贝尔曼方程（Bellman Equation）**：  
  $$
  V^\pi(s) = \sum_a \pi(a \mid s) \sum_{s', r} p(s', r \mid s, a) \left[ r + \gamma V^\pi(s') \right]
  $$
  表示当前状态价值等于即时奖励加上后继状态的折扣价值期望。

---

### **2. 动作价值函数（Action-Value Function, \( Q^\pi(s, a) \)）**
- **定义**：在策略 \( \pi \) 下，从状态 \( s \) 执行动作 \( a \) 后，智能体未来获得的**期望回报**。  
  $$
  Q^\pi(s, a) = \mathbb{E}_\pi \left[ G_t \mid S_t = s, A_t = a \right]
  $$
  
  
- **特点**：  
  - 同时依赖状态 \( s \) 和动作 \( a \)，直接关联策略的决策。  
  - 用于优化策略（例如：选择使 \( Q \) 最大的动作，如Q-Learning）。  

- **贝尔曼方程**：  
  $$
  Q^\pi(s, a) = \sum_{s', r} p(s', r \mid s, a) \left[ r + \gamma \sum_{a'} \pi(a' \mid s') Q^\pi(s', a') \right]
  $$
  

---

### **3. 两者的关系**
1. **从 \( Q^\pi \) 推导 \( V^\pi \)**：  
   $$
   V^\pi(s) = \sum_a \pi(a \mid s) Q^\pi(s, a)
   $$
   即状态价值是动作价值的加权平均（权重为策略 \( \pi \) 的动作概率）。

2. **从 \( V^\pi \) 推导 \( Q^\pi \)**（已知环境模型时）：  
   $$
   Q^\pi(s, a) = \sum_{s', r} p(s', r \mid s, a) \left[ r + \gamma V^\pi(s') \right]
   $$

### **示例**
假设一个网格世界中：  
- **状态 \( s \)**：智能体当前位置。  
- **动作 \( a \)**：上、下、左、右移动。  
- **\( V^\pi(s) \)**：从当前位置出发，按策略 \( \pi \) 能获得的总奖励期望。  
- **\( Q^\pi(s, a) \)**：从当前位置选择动作 \( a \) 后，再按 \( \pi \) 行动的总奖励期望。

