# AC概念学习

**我们可以把状态价值函数V作为基线，从Q函数减去这个V函数则得到了A函数，我们称之为优势函数（advantage function）**事实上，用Q值或者V值本质上也是用奖励来进行指导，但是用神经网络进行估计的方法可以减小方差、提高鲁棒性。

策略梯度写成一下格式
$$
g = \mathbb{E} \left[ \sum_{t=0}^{T} \psi_t \nabla_\theta \log \pi_\theta(a_t | s_t) \right]
$$

1. \sum_{t'=0}^{T} \gamma^{t'} r_{t'}: 轨迹的总回报；

2. \(\sum_{t'=t}^{T} \gamma^{t'-t} r_{t'}\): 动作 \(a_t\) 之后的回报；

3. \(\sum_{t'=t}^{T} \gamma^{t'-t} r_{t'} - b(s_t)\): 基准线版本的改进；

4. \(Q^{\pi_\theta}(s_t, a_t)\): 动作价值函数；

5. \(A^{\pi_\theta}(s_t, a_t)\): 优势函数；

6. \(r_t + \gamma V^{\pi_\theta}(s_{t+1}) - V^{\pi_\theta}(s_t)\): 时序差分残差。

Actor 要做的是与环境交互，**并在 Critic 价值函数的指导下用策略梯度学习一个更好的策略**

**Critic 要做的是通过 Actor 与环境交互收集的数据学习一个价值函数V**，这个价值函数会用于判断在当前状态什么动作是好的，什么动作不是好的，进而帮助 Actor 进行策略更新。



# 代码学习

```
action_dist = torch.distributions.Categorical(probs)
```

这行代码是在使用 PyTorch 中的 `torch.distributions.Categorical` 类来创建一个类别分布对象。具体来说，这个对象用于表示一个离散的概率分布，可以用于在给定的概率下采样离散的动作或值。下面是对这段代码的解释：

1. **`torch.distributions.Categorical`**：这是 PyTorch 提供的一个用于创建类别（离散）分布的类。类别分布是指在有限个离散事件中，每个事件发生的概率都被显式地定义。

2. **`probs`**：这个变量通常是一个一维的张量，包含了每个类别的概率。所有的概率值加起来应该等于1。这个张量定义了分布的形状和每个事件的发生概率。

3. **`action_dist`**：这是创建的类别分布对象。通过这个对象，你可以进行多种操作，例如：
   - **`sample()`**：从该分布中随机采样一个事件（根据定义的概率）。
   - **`log_prob(value)`**：计算某个具体事件的对数概率。
   - **`entropy()`**：计算分布的熵，衡量分布的不确定性程度。

使用这个分布对象在强化学习和其他机器学习任务中是很常见的，特别是在策略梯度方法中，用于表示和采样策略的动作。通过定义这样的分布，可以在离散的动作空间中进行有效的探索和学习。



```python
actions = torch.tensor(transition_dict['actions']).view(-1, 1).to(self.device)
```

在 PyTorch 中，`view(-1, 1)` 用于对张量进行重塑操作，其中 `-1` 表示自动推断该维度的大小，`1` 则表示将张量的形状调整为列向量。以下是对每个部分使用 `view(-1, 1)` 的解释：

1. **`actions`、`rewards` 和 `dones`**：
   - 这些张量都被重塑为二维的列向量形状，形状为 `(N, 1)`，其中 `N` 是样本的数量。
   
   - **原因**：在强化学习中，通常需要将这些数据与其他矩阵进行矩阵运算（如批量计算损失、更新策略等）。为了确保矩阵维度一致（特别是在计算损失或更新参数时），需要将这些一维数据转换为二维的列向量。
   
   - **方便进行批处理**
   
2. **`actions`**：
   
   - 动作通常是一个离散的整数值，在构建批量数据时，将其作为列向量便于后续的运算或与其他张量进行拼接。

3. **`rewards` 和 `dones`**：
   - 奖励和终止标志本质上是标量值，但在批量操作中，以列向量形式存储有利于与其他批量数据保持一致性。
   
4. **为什么不用在 `states` 和 `next_states` 上使用 `view(-1, 1)`**：
   
   - `states` 和 `next_states` 通常是多维的（例如，图像、环境状态等），其形状通常已经是 `(N, D)`，其中 `D` 是状态的维度，因此不需要额外的重塑操作。

通过使用 `view(-1, 1)`，确保了在批量梯度更新或损失计算中，各个张量的形状与计算图兼容，避免了潜在的维度不匹配问题。



```
td_target = rewards + self.gamma * self.critic(next_states) * (1 - dones)
```

done 表示最后没有最后一个状态了，只有reward奖励



```
log_probs = torch.log(self.actor(states).gather(1, actions))
```

可以近似为批处理方式

这行代码涉及到从策略网络中获取动作的概率，并计算其对数概率。下面是对这行代码的详细解释：

1. **`self.actor(states)`**:
   - 这里 `self.actor` 是一个神经网络模型，通常是用于策略梯度方法中的策略网络。在强化学习中，策略网络接收当前的状态 `states` 作为输入，并输出每个可能动作的概率分布。
   - `states` 是一个包含多个状态的张量，通常形状为 `(batch_size, state_dim)`，其中 `batch_size` 是批量的大小，`state_dim` 是状态的维度。

2. **`.gather(1, actions)`**:
   - `self.actor(states)` 返回的输出是一个二维张量，每行代表一个状态下所有可能动作的概率分布。
   - `actions` 是一个列向量，形状为 `(batch_size, 1)`，其中每个元素表示在对应状态下选择的动作索引。
   - `gather(1, actions)` 的作用是从每行中选择出在 `actions` 所指定的索引上的值。也就是说，对于每个状态，选择出在该状态下执行的动作的概率。

3. **`torch.log(...)`**:
   - `torch.log` 用于计算选定动作的概率的自然对数。对数概率在许多机器学习算法中都很有用，因为对数函数将概率（通常是小数）转换为对数空间，降低了数值计算的不稳定性。
   - 在策略梯度方法中，使用对数概率可以简化梯度计算，这对优化过程是有利的。

**总结来说，这行代码的目的是从策略网络中得到给定状态下选择的动作的概率，然后计算该概率的对数。这样可以在策略梯度更新中用于计算损失函数，比如在策略梯度方法中使用的损失函数中，通常需要用到对数概率。**



## td误差

```
# 时序差分目标
td_target = rewards + self.gamma * self.critic(next_states) * (1 - dones)                            
td_delta = td_target - self.critic(states)  # 时序差分误差
```

在强化学习中，时序差分（Temporal Difference, TD）方法是一种重要的技术，用于估计价值函数。这两行代码与 Actor-Critic 算法有关，这是一个结合了策略梯度和价值函数逼近的强化学习方法。

让我们逐行分析这些代码的含义：

### 1. 时序差分目标 (`td_target`)

```python
td_target = rewards + self.gamma * self.critic(next_states) * (1 - dones)
```

- `rewards`：这是当前时间步获得的即时奖励。

- `self.gamma`：这是折扣因子（通常在 0 到 1 之间）。它用于平衡当前奖励和未来奖励的相对重要性。较小的 `gamma` 值会让代理更关注短期奖励，而较大的值则让代理更关注长期奖励。

- `self.critic(next_states)`：这表示对下一个状态的价值的估计。`critic` 是价值函数的近似器（可能是一个神经网络），用于估计状态的价值。

- `(1 - dones)`：这是一个布尔掩码，用于处理终止状态（结束状态）。如果一个状态是终止状态（`done` 为 `True`），则 `1 - dones` 为 0，这样可以确保在终止状态不再考虑未来的奖励。这是因为在终止状态后没有更多的未来状态。

**这行代码的目标是计算时序差分目标 `td_target`，它表示当前状态的“真实”价值。这个值是通过即时奖励加上下一个状态的折扣价值来计算的。**

### 2. 时序差分误差 (`td_delta`)

```python
td_delta = td_target - self.critic(states)
```

- `td_target`：这是刚刚计算出的时序差分目标。

- `self.critic(states)`：这表示对当前状态的价值的估计。

时序差分误差 `td_delta` 是实际观察到的值（`td_target`）与当前策略下估计的值（`self.critic(states)）之间的差异。这个误差用于更新策略和价值函数，以减少估计误差。

### 总结

在 Actor-Critic 方法中：

- `td_target` 是期望的价值估计，结合了即时奖励和未来的折扣价值。
- `td_delta` 是用来衡量当前策略下的估计误差。这个误差用于调整策略（通过 Actor）和价值函数（通过 Critic）。

通过不断减少 `td_delta`，算法逐步改进策略，使其在环境中表现得越来越好。



## actor的loss计算原因

```
actor_loss = torch.mean(-log_probs * td_delta.detach())
```

这行代码计算了策略梯度方法中的策略损失（`actor_loss`），并通过最小化这个损失来更新策略网络（`actor`）。让我们详细解析这行代码：

1. **`log_probs`**:
   - 这是从策略网络输出的对数概率。具体来说，是在给定状态下，选择实际执行动作的对数概率。

2. **`td_delta.detach()`**:
   - `td_delta` 是时间差误差（Temporal Difference Error），通常用于衡量某个动作相对于当前策略的好坏。它可能来自于某种形式的优势函数估计（如 TD(0) 误差、GAE 等）。
   - `detach()` 的作用是从计算图中分离出 `td_delta`，即在计算 `actor_loss` 的梯度时不考虑 `td_delta` 的梯度。这是因为我们在更新策略时，不希望 `td_delta` 的值受到策略网络参数更新的影响。

3. **`-log_probs * td_delta.detach()`**:
   - 这部分计算了对于每个样本的损失值。`-log_probs` 是因为我们在进行梯度上升（最大化回报），但 PyTorch 的优化器默认进行的是梯度下降，所以我们加一个负号。
   - `log_probs * td_delta.detach()` 表示我们希望增大选择了好的动作（增大 `td_delta`）的概率，减小选择了不好的动作的概率。

4. **`torch.mean(...)`**:
   - 计算所有样本损失的平均值，这是为了得到一个标量损失值，便于进行反向传播和参数更新。

### 整体作用：
1. **策略梯度（Policy Gradient）**：
   - 策略梯度方法的目标是直接优化策略参数，使得执行策略的期望累积奖励最大化。
   - 基于策略梯度定理，策略的梯度可以用选定动作的对数概率与对应的优势（Advantage）乘积来表示。这种方法可以有效地对策略进行优化。

2. **优势函数（Advantage Function）**：
   - 优势函数 \( A(s, a) \) 衡量一个动作相对于某个状态的平均水平有多好。通常，它是由状态动作值函数 \( Q(s, a) \) 减去状态值函数 \( V(s) \) 得到的，即 \( A(s, a) = Q(s, a) - V(s) \)。
   - 使用优势函数可以帮助我们判断一个动作在给定状态下是否比平均水平更好（正优势）或者更差（负优势）。

3. **使用 TD-Error 作为优势估计**：
   - 在 Actor-Critic 方法中，时间差分误差（TD Error）通常被用作优势函数的近似，即 $$\delta = r + \gamma V(s') - V(s)$$。它表示当前状态下执行动作后，得到的即时奖励与估计的状态值之间的差异。
   - 使用 TD-Error 作为优势估计可以减少方差，同时保持策略梯度的无偏性。

4. **损失函数的形式**：
   - 损失函数形式为 $$-\log \pi(a|s) \cdot A(s, a)$$，对于每个样本，$$-\log \pi(a|s)$$ 是选择动作 \(a\) 的对数概率的负值。
   - **如果 \( A(s, a) > 0 \)，即动作比平均水平好，那么通过减小负对数概率（即增大概率），可以增加选择此动作的倾向。**
   - **如果 \( A(s, a) < 0 \)，即动作比平均水平差，那么通过增大负对数概率（即减少概率），可以减少选择此动作的倾向。**

5. **整体目标**：
   - 通过这种损失函数，策略网络能够逐渐学习在不同状态下选择最优动作的概率分布。这种方法结合了策略梯度的优点（直接优化策略）和价值函数的优点（减少方差），从而在许多任务中表现出色。

综上所述，Actor-Critic 方法中的 Actor 采用这种损失函数是为了有效地利用策略梯度和价值函数的信息，指导策略网络学习更优的动作选择策略。