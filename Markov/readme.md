```
values = np.dot(np.linalg.inv(np.eye(states_num, states_num) - gamma * P), rewards)
```

这段代码使用 NumPy 库计算马尔可夫决策过程（MDP）中的状态值函数，给定转移概率矩阵和奖励。具体来说，这段代码用来解决一个无折扣的马尔可夫奖励过程（MRP）或者一个折扣的马尔可夫决策过程（MDP）的贝尔曼期望方程。

### 代码解释

- `np.eye(states_num, states_num)`: 
  - 生成一个大小为 `states_num x states_num` 的单位矩阵。单位矩阵是一个对角线元素为 1，其它元素为 0 的方阵。

- `gamma`: 
  - 折扣因子（通常 `0 <= gamma < 1`），用于衡量未来奖励的重要性。`gamma` 越小，未来奖励的重要性越低。

- `P`: 
  - 转移概率矩阵，大小为 `states_num x states_num`，表示从一个状态转移到另一个状态的概率。

- `rewards`: 
  - 奖励向量，大小为 `states_num`，表示每个状态的即时奖励。

- `np.linalg.inv(...)`: 
  - 计算矩阵的逆。

- `np.dot(...)`: 
  - 计算矩阵乘法。

### 计算过程

1. **矩阵求解**：
   - `np.eye(states_num, states_num) - gamma * P`：计算矩阵 `(I - γP)`，其中 `I` 是单位矩阵。
   - `np.linalg.inv(...)`: 计算矩阵 `(I - γP)` 的逆。这个逆矩阵用于求解贝尔曼期望方程。

2. **状态值计算**：
   - `np.dot(...)`: 将逆矩阵与奖励向量 `rewards` 相乘，得到状态值函数 `values`。即 `values = (I - γP)^(-1) * rewards`。
   - 结果 `values`: 一个包含每个状态值的向量，表示在该状态下开始时的期望总奖励。

### 应用

这段代码适用于计算给定 MRP 或 MDP 的状态值函数，特别是在确定性策略下。通过求解贝尔曼期望方程，可以得到每个状态的长期价值，这对于策略评估和改进都很有用。



# 占用度量 occupancy

在这段文字中，主要讨论了马尔可夫决策过程（MDP）中的状态访问分布和策略的占用度量。以下是公式的解释：

### 状态访问分布（State Visitation Distribution）

- **公式**：\[ \nu^\pi(s) = (1 - \gamma) \sum_{t=0}^{\infty} \gamma^t P_t^\pi(s) \]

- **解释**：
  - \(\nu^\pi(s)\) 表示在策略 \(\pi\) 下访问状态 \(s\) 的概率。
  - \(P_t^\pi(s)\) 是在策略 \(\pi\) 下经过 \(t\) 步后到达状态 \(s\) 的概率。
  - \((1 - \gamma)\) 是归一化因子，确保总概率和为 1。
  - 这个公式表示在策略 \(\pi\) 下，MDP 在每一步访问状态 \(s\) 的概率，总和即为状态访问分布。

- **性质**：
  - 状态访问分布反映了在策略与 MDP 交互时访问不同状态的频率。

### 占用度量（Occupancy Measure）

- **公式**：\[ \rho^\pi(s, a) = (1 - \gamma) \sum_{t=0}^{\infty} \gamma^t P_t^\pi(s) \pi(a|s) \]

- **解释**：
  - \(\rho^\pi(s, a)\) 表示在策略 \(\pi\) 下，MDP 在状态 \(s\) 采取动作 \(a\) 的频率。
  - \(\pi(a|s)\) 是在状态 \(s\) 下选择动作 \(a\) 的概率。
  - 这个度量结合了状态访问分布和策略选择，表示在整个过程中某状态和动作的占用情况。

- **性质**：
  - 占用度量用于衡量在给定策略下，状态和动作对的访问频率。

### 关系

- \(\rho^\pi(s, a) = \nu^\pi(s) \pi(a|s)\)：状态访问分布和策略选择概率的乘积，表示在给定状态下采取某动作的频率。

这些公式和概念对于理解策略在MDP中的表现和优化策略非常重要。通过这些度量，可以更好地分析和改进智能体的决策过程。



## 归一化因子

在马尔可夫决策过程（MDP）中，\((1-\gamma)\) 作为归一化因子是为了确保状态访问分布的总和为 1。这是因为：

### γ的作用

- \(\gamma\) 是折扣因子，通常 \(0 \leq \gamma < 1\)，用于给未来的奖励打折。
- 当 \(\gamma\) 较小时，未来的奖励和访问概率的权重会减小。

### 归一化因子的作用

- 在无穷求和 \(\sum_{t=0}^{\infty} \gamma^t P_t^\pi(s)\) 中，如果没有 \((1-\gamma)\)，总和可能超过 1，因为它是一个无限级数。
  
- \((1-\gamma)\) 确保在 \(\gamma < 1\) 的情况下，整个级数的总和是有限的，并且归一化为 1。实际上，\(\sum_{t=0}^{\infty} \gamma^t = \frac{1}{1-\gamma}\) 是一个几何级数的和。

因此，\((1-\gamma)\) 作为归一化因子，确保在策略 \(\pi\) 下访问所有状态的概率分布是一个有效的概率分布（概率总和为 1）。这使得状态访问分布可以被合理地解释为概率。



## 归一化因子

归一化因子是用于调整一个向量或分布，使其符合某种标准或约束。对于概率分布，归一化因子确保所有概率之和为 1。下面是关于归一化因子的详细解释：

### 归一化因子的作用

1. **调整到标准范围**：
   - 确保总和满足特定要求，比如概率总和为 1。

2. **保持比例关系**：
   - 在调整过程中保持各个元素的相对大小不变。

3. **确保有效性**：
   - 保证结果符合所需的数学性质，例如概率的有效性。

### 几何级数中的归一化

在几何级数中，像 \(\sum_{t=0}^{\infty} \gamma^t\)，其和为 \(\frac{1}{1-\gamma}\)，如果没有归一化因子 \((1-\gamma)\)，则这个和可能会使概率总和超过 1。因此，\((1-\gamma)\) 被用来调整，使整个和为 1。

### 应用场景

- **概率分布**：调整一组数，使其总和为 1。
- **数据标准化**：在机器学习中，把数据调整到一个标准范围，比如 0 到 1。

通过使用归一化因子，可以确保计算结果符合预期的数学和统计性质。